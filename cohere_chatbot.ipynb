{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install langchain faiss-cpu cohere\n",
    "# pip install langchain-community langchain-cohere python-dotenv\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "COHERE_API_KEY = os.getenv(\"COHERE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "knowledge_base_text = \"\"\"\n",
    "LangChain is an open-source framework designed to help developers build applications powered by large language models.\n",
    "It provides tools for chaining prompts, managing memory, and integrating with external data sources.\n",
    "Python is a versatile programming language used widely in data science, AI, web development, and more.\n",
    "Generative AI refers to a class of artificial intelligence models capable of generating text, images, or other data types based on input prompts.\n",
    "\"\"\"\n",
    "\n",
    "with open(\"knowledge_base.txt\", \"w\") as file:\n",
    "    file.write(knowledge_base_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader('knowledge_base.txt')\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_cohere import CohereEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "embedding = CohereEmbeddings(cohere_api_key = COHERE_API_KEY, model = \"embed-english-v3.0\")\n",
    "vectorstore = FAISS.from_documents(documents, embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_cohere import ChatCohere\n",
    "\n",
    "chat_model = ChatCohere(\n",
    "    model = \"command-r-plus\",\n",
    "    cohere_api_key = COHERE_API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef chatbot():\\n    print(\"Message LangChain AI Chatbot. Type \\'exit\\' to end the chat.\")\\n    while True:\\n        query = input(\"You: \")\\n        if query.lower() == \\'exit\\':\\n            print(\"Goodbye!\")\\n            break\\n        \\n        # relevant context from the knowledge base\\n        docs = vectorstore.similarity_search(query, k = 1)\\n        context = docs[0].page_content if docs else \"No relevant information found.\"\\n        \\n        prompt = f\"Based on the following context, answer the user\\'s query:\\n\\nContext: {context}\\n\\nQuery: {query}\"\\n        user_message = HumanMessage(content = prompt)\\n        \\n        response = chat_model.invoke([user_message])\\n        print(f\"AI: {response.content}\")\\n\\nchatbot()\\n'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "'''\n",
    "def chatbot():\n",
    "    print(\"Message LangChain AI Chatbot. Type 'exit' to end the chat.\")\n",
    "    while True:\n",
    "        query = input(\"You: \")\n",
    "        if query.lower() == 'exit':\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        # relevant context from the knowledge base\n",
    "        docs = vectorstore.similarity_search(query, k = 1)\n",
    "        context = docs[0].page_content if docs else \"No relevant information found.\"\n",
    "        \n",
    "        prompt = f\"Based on the following context, answer the user's query:\\n\\nContext: {context}\\n\\nQuery: {query}\"\n",
    "        user_message = HumanMessage(content = prompt)\n",
    "        \n",
    "        response = chat_model.invoke([user_message])\n",
    "        print(f\"AI: {response.content}\")\n",
    "\n",
    "chatbot()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmemory = ConversationBufferMemory()\\n\\ndef chatbot_with_memory():\\n    print(\"Message LangChain AI Chatbot with Memory. Type \\'exit\\' to end the chat.\")\\n    while True:\\n        query = input(\"You: \")\\n        if query.lower() == \\'exit\\':\\n            print(\"Goodbye!\")\\n            break\\n        \\n        docs = vectorstore.similarity_search(query, k = 1)\\n        context = docs[0].page_content if docs else \"No relevant information found.\"\\n        \\n        # adding context and query to memory\\n        memory.chat_memory.add_user_message(query)\\n        memory.chat_memory.add_ai_message(f\"Context: {context}\")\\n        \\n        prompt = f\"Conversation History:\\n{memory.buffer}\\n\\nBased on the following context, answer the user\\'s query:\\n\\nContext: {context}\\n\\nQuery: {query}\"\\n        user_message = HumanMessage(content = prompt)\\n        response = chat_model.invoke([user_message])\\n        \\n        # storing response\\n        memory.chat_memory.add_ai_message(response.content)\\n        \\n        print(f\"AI: {response.content}\")\\n\\nchatbot_with_memory()\\n'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "'''\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "def chatbot_with_memory():\n",
    "    print(\"Message LangChain AI Chatbot with Memory. Type 'exit' to end the chat.\")\n",
    "    while True:\n",
    "        query = input(\"You: \")\n",
    "        if query.lower() == 'exit':\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        docs = vectorstore.similarity_search(query, k = 1)\n",
    "        context = docs[0].page_content if docs else \"No relevant information found.\"\n",
    "        \n",
    "        # adding context and query to memory\n",
    "        memory.chat_memory.add_user_message(query)\n",
    "        memory.chat_memory.add_ai_message(f\"Context: {context}\")\n",
    "        \n",
    "        prompt = f\"Conversation History:\\n{memory.buffer}\\n\\nBased on the following context, answer the user's query:\\n\\nContext: {context}\\n\\nQuery: {query}\"\n",
    "        user_message = HumanMessage(content = prompt)\n",
    "        response = chat_model.invoke([user_message])\n",
    "        \n",
    "        # storing response\n",
    "        memory.chat_memory.add_ai_message(response.content)\n",
    "        \n",
    "        print(f\"AI: {response.content}\")\n",
    "\n",
    "chatbot_with_memory()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Testing Non-Memory Chatbot ---\n",
      "You: I used LangChain back in 2022 for a personal project at my university\n",
      "AI: Yes, LangChain has been available as an open-source framework since 2021. So, it is very possible that you used LangChain for your personal project at the university in 2022.\n",
      "\n",
      "You: When have I used LangChain before?\n",
      "AI: You have not mentioned any specific instances of using LangChain in your provided context. However, based on the information given, it can be inferred that you might have used LangChain if you were a developer working on building applications powered by large language models and required a framework to help with prompt chaining, memory management, and data integration.\n",
      "\n",
      "\n",
      "--- Testing Memory Chatbot ---\n",
      "You: I used LangChain back in 2022 for a personal project at my university\n",
      "AI: It's great to hear that you've had experience with LangChain! It's an excellent tool for developers looking to harness the power of large language models. Was your personal project a success?\n",
      "\n",
      "You: When have I used LangChain before?\n",
      "AI: You mentioned that you used LangChain back in 2022 for a personal project at your university.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def test_non_memory_chatbot():\n",
    "    queries = [\n",
    "        \"I used LangChain back in 2022 for a personal project at my university\",\n",
    "        \"When have I used LangChain before?\"\n",
    "    ]\n",
    "    print(\"\\n--- Testing Non-Memory Chatbot ---\")\n",
    "    for query in queries:\n",
    "        docs = vectorstore.similarity_search(query, k = 1)\n",
    "        context = docs[0].page_content if docs else \"No relevant information found.\"\n",
    "        \n",
    "        prompt = f\"Based on the following context, answer the user's query:\\n\\nContext: {context}\\n\\nQuery: {query}\"\n",
    "        user_message = HumanMessage(content = prompt)\n",
    "        response = chat_model.invoke([user_message])\n",
    "        print(f\"You: {query}\\nAI: {response.content}\\n\")\n",
    "\n",
    "def test_memory_chatbot():\n",
    "    queries = [\n",
    "        \"I used LangChain back in 2022 for a personal project at my university\",\n",
    "        \"When have I used LangChain before?\"\n",
    "    ]\n",
    "    print(\"\\n--- Testing Memory Chatbot ---\")\n",
    "\n",
    "    memory = ConversationBufferMemory()\n",
    "\n",
    "    for query in queries:\n",
    "        docs = vectorstore.similarity_search(query, k = 1)\n",
    "        context = docs[0].page_content if docs else \"No relevant information found.\"\n",
    "                \n",
    "        memory.chat_memory.add_user_message(query)\n",
    "        memory.chat_memory.add_ai_message(f\"Context: {context}\")\n",
    "        \n",
    "        prompt = f\"Conversation History:\\n{memory.buffer}\\n\\nBased on the following context, answer the user's query:\\n\\nContext: {context}\\n\\nQuery: {query}\"\n",
    "        user_message = HumanMessage(content = prompt)\n",
    "        response = chat_model.invoke([user_message])\n",
    "        \n",
    "        memory.chat_memory.add_ai_message(response.content)\n",
    "        print(f\"You: {query}\\nAI: {response.content}\\n\")\n",
    "\n",
    "test_non_memory_chatbot()\n",
    "test_memory_chatbot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
